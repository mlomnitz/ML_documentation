{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree's\n",
    "\n",
    "Decision tree's are a supervised algorith used for both regression and classification.\n",
    "\n",
    "### Pros:\n",
    "- Simple to understand and interpret. the tree can be visualized and decisions leading to specific outcomes can be tracked\n",
    "- Require little preprocessing compared to other procedures. They can even handle blank data or missing values. (This is not true for the [scikit-learn](http://scikit-learn.org/stable/modules/tree.html) implementation).\n",
    "- Compute is expensive: logarithmic in terms of the data used to train the model.\n",
    "- Good for both numerical and categorical data.\n",
    "\n",
    "### Cons:\n",
    "- Prone to overfitting, especially for deep tree's.\n",
    "- Not always robust. they are susceptible to small fluctuations in the data. this is remedied using ensemble methods (random forest).\n",
    "- Only captures linear combinations of features.\n",
    "- NP-Complete: This is a greedy algorithm that optimizes each node individiaually so there is no assurance that the minima obtained is close to global optimum.\n",
    "- Can create biased trees if the data is imbalanced --> it is important to balance your training data. \n",
    "\n",
    "### How they work:\n",
    "\n",
    "1.) Recursively parittions the data using a selected feeature and threshold with the objective that data with the same label are on the same branch of the tree.\n",
    "\n",
    "2.) For a given partition (feature _j_, threshold _tm_) calculate the impurity (i.e. the proportion of objects from class _k_ that fall in class _m_).\n",
    "\n",
    "3.) Find and chose the partition that minimizes this impurity.\n",
    "\n",
    "4.) Repeat the procedure with new branches till the tree depth is reached.\n",
    "\n",
    "### Classification criteria\n",
    "\n",
    "If a target classification outcome is given by the proportion of objects from class _k_ that are classified as being in class _m_, we have:\n",
    "\n",
    "<img src=\"ml_doc_images/impurity.png\" width=\"250px\"/>\n",
    "\n",
    "Then the following are commonly used criteria for classification\n",
    "\n",
    "- Gini index: \n",
    "\n",
    "<img src=\"ml_doc_images/Gini.png\" width=\"250px\"/>\n",
    "\n",
    "- Cross entropy\n",
    "\n",
    "<img src=\"ml_doc_images/CrossEntropy.png\" width=\"250px\"/>\n",
    "\n",
    "- Mis-classification\n",
    "\n",
    "<img src=\"ml_doc_images/Misclass.png\" width=\"220px\"/>\n",
    "\n",
    "### Regression criteria\n",
    "\n",
    "For continuos variables, for mode _m_ with _N_ measurments a common way to determine locations for future split si trying to minimise either L1 or L2 norm using the mean values at terminal nodes\n",
    "\n",
    "- L2 norm\n",
    "\n",
    "<img src=\"ml_doc_images/L2.png\" width=\"250px\"/>\n",
    "\n",
    "- L1 norm\n",
    "\n",
    "<img src=\"ml_doc_images/L1.png\" width=\"250px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples copied from scikit-learn page\n",
    "\n",
    "### Classification (in sklearn)\n",
    "\n",
    "As with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of size [n_samples, n_features] holding the training samples, and an array Y of integer values, size [n_samples], holding the class labels for the training samples:\n",
    "\n",
    "```python\n",
    ">>> from sklearn import tree\n",
    ">>> X = [[0, 0], [1, 1]]\n",
    ">>> Y = [0, 1]\n",
    ">>> clf = tree.DecisionTreeClassifier()\n",
    ">>> clf = clf.fit(X, Y)\n",
    "```\n",
    "After being fitted, the model can then be used to predict the class of samples:\n",
    "\n",
    "``` python\n",
    ">>> clf.predict([[2., 2.]])\n",
    "array([1])\n",
    "```\n",
    "Alternatively, the probability of each class can be predicted:\n",
    "\n",
    "```python\n",
    ">>> clf.predict_proba([[2., 2.]])\n",
    "array([[ 0.,  1.]])\n",
    "```\n",
    "DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, …, K-1]) classification.\n",
    "\n",
    "Using the Iris dataset, we can construct a tree as follows:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn import tree\n",
    ">>> iris = load_iris()\n",
    ">>> clf = tree.DecisionTreeClassifier()\n",
    ">>> clf = clf.fit(iris.data, iris.target)\n",
    "```\n",
    "Once trained, we can export the tree in Graphviz format using the export_graphviz exporter.\n",
    "\n",
    "Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file iris.pdf:\n",
    "\n",
    "```python\n",
    ">>> import graphviz \n",
    ">>> dot_data = tree.export_graphviz(clf, out_file=None) \n",
    ">>> graph = graphviz.Source(dot_data) \n",
    ">>> graph.render(\"iris\") \n",
    "```\n",
    "\n",
    "Jupyter notebooks also render these plots inline automatically:\n",
    "\n",
    "```python\n",
    ">>> dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    ">>> graph = graphviz.Source(dot_data)  \n",
    ">>> graph \n",
    "```\n",
    "\n",
    "### Regression\n",
    "\n",
    "Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.\n",
    "\n",
    "As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:\n",
    "\n",
    "```python\n",
    ">>> from sklearn import tree\n",
    ">>> X = [[0, 0], [2, 2]]\n",
    ">>> y = [0.5, 2.5]\n",
    ">>> clf = tree.DecisionTreeRegressor()\n",
    ">>> clf = clf.fit(X, y)\n",
    ">>> clf.predict([[1, 1]])\n",
    "array([ 0.5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "A random forest is an ensemble method that tries to fit dataset using multiple decision trees trained on a subset of the data, and use the average result to improve accuracy and reduce effects of over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted Trees and AdaBoost\n",
    "\n",
    "Gradient boosted trees use a series of weak (shallow) trees/learnes where the output of one tree is the input of the subsequent tree, and they are used to fit on the residuals.\n",
    "\n",
    "The advantages of GBRT are:\n",
    "\n",
    "- Natural handling of data of mixed type (= heterogeneous features)\n",
    "- Predictive power\n",
    "- Robustness to outliers in output space (via robust loss functions)\n",
    "The disadvantages of GBRT are:\n",
    "\n",
    "- Scalability, due to the sequential nature of boosting it can hardly be parallelized.\n",
    "\n",
    "GradientBoostingClassifier supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners:\n",
    "### Classification\n",
    "```python\n",
    ">>> from sklearn.datasets import make_hastie_10_2\n",
    ">>> from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    ">>> X, y = make_hastie_10_2(random_state=0)\n",
    ">>> X_train, X_test = X[:2000], X[2000:]\n",
    ">>> y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    ">>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    ">>> clf.score(X_test, y_test)                 \n",
    "0.913...\n",
    "```\n",
    "### Regression\n",
    "GradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls').\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    ">>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    ">>> X_train, X_test = X[:200], X[200:]\n",
    ">>> y_train, y_test = y[:200], y[200:]\n",
    ">>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    ">>> mean_squared_error(y_test, est.predict(X_test))    \n",
    "5.00...\n",
    "```\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "In the case of AdaBoost, a series of models are trained as in GBT but the weights of the nodes are changed over time according to their discriminative power and the predictions are done with the wieghted average of the trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
