{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes are a set of supervised algorthithms based on applying Baye's theorem for conditional probabilities, with he 'naive' coming the from assumption that the different features are independent form each other. Given a class variable _y_ and a set of n features _xn_, a naive Bayes states that:\n",
    "\n",
    "<img src=\"ml_doc_images/NaiveBayes.png\" width=\"600pix\" />\n",
    "\n",
    "We then use the max a posteriori (MAP) to estimate P(y) and P(xi|y), where the former is the relative frequency of y in the training set. \n",
    "  \n",
    "There are mutliple Naive Bayes algorithms which differ, in essence, due to their assumption for the orm of P(xi|y).\n",
    "\n",
    "### Pros\n",
    "- Simple and easy to implement.\n",
    "- In spite of their simplicity Naive Bayes approaches have been succesful in many applications such as **document classification** and **spam filtering**.\n",
    "- Can be used wit limited training samples.\n",
    "- Fast training and inference.\n",
    "\n",
    "### Cons\n",
    "- Limited due to their assumptions (i.e. simplicity)\n",
    "- Even though Naive Bayes might be a descent classifier it is a known poor estimator, so the output probabilities are not to be trusted.\n",
    "\n",
    "The following section illustrates ome of the probailities that are implemented in [scikit-learn](http://scikit-learn.org/stable/modules/naive_bayes.html) toguether with some sample implementations.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "The parameters \\sigma_y and \\mu_y are estimated using maximum likelihood.\n",
    "\n",
    "<img src=\"ml_doc_images/GaussNaive.png\" width = \"300px\"\\>\n",
    "\n",
    "```python\n",
    ">>> from sklearn import datasets\n",
    ">>> iris = datasets.load_iris()\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> gnb = GaussianNB()\n",
    ">>> y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    ">>> print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n",
    "Number of mislabeled points out of a total 150 points : 6\n",
    "```\n",
    "\n",
    "### Multinomial Bayes\n",
    "\n",
    "Naive Bayes algorithm for multinomially distributed data, is one of the two classic naive Bayes variants used in text classification. The distribution is parametrized by vectors _\\theta_y_ for each class y and n features (e.g. vocabulary size). The parameters _\\theta_y_ are estimated by a smoothed frequency count\n",
    "\n",
    "<img src=\"ml_doc_images/MultinomialNaive.png\" width = \"700px\" \\>\n",
    "\n",
    "### Bernoulli Bayes\n",
    "\n",
    "This implements Naive Bayes training and classification using a multinomial Bernoulli. There may be several features but each is taken to be bniary-valued. Therefore, this class requires samples to be represented as binary-valued feature vectors.\n",
    "\n",
    "The decision rule for Bernoulli naive Bayes is based on\n",
    "\n",
    "<img src=\"ml_doc_images/BernoulliNaive.png\" width=\"340px\" \\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
